<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.18">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed"><title data-rh="true">transformer | My Site</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://sileaver.github.io/blog/2022/05/12/transformer"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="transformer | My Site"><meta data-rh="true" name="description" content="[TOC]"><meta data-rh="true" property="og:description" content="[TOC]"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2022-05-12T00:00:00.000Z"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://sileaver.github.io/blog/2022/05/12/transformer"><link data-rh="true" rel="alternate" href="https://sileaver.github.io/blog/2022/05/12/transformer" hreflang="en"><link data-rh="true" rel="alternate" href="https://sileaver.github.io/blog/2022/05/12/transformer" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.5627add0.css">
<link rel="preload" href="/assets/js/runtime~main.6adc9f19.js" as="script">
<link rel="preload" href="/assets/js/main.4372d31a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">My Site</b></a><a class="navbar__item navbar__link" href="/docs/intro">Tutorial</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_S7eR colorModeToggle_vKtC"><button class="clean-btn toggleButton_rCf9 toggleButtonDisabled_Pu9x" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/2022/06/15/test">test</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/2022/6/14/jupyter">jupyter</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/2022/6/13/pythonclass">pythonclass</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/2022/05/28/BayesDecisionTheory">BayesDecisionTheory</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/2022/05/25/BMLCodelab">BMLCodelab</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_rzP5" itemprop="headline">transformer</h1><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-05-12T00:00:00.000Z" itemprop="datePublished">May 12, 2022</time> · <!-- -->9 min read</div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>[TOC]</p><p>这东西老实说是一个大坑，坑在哪呢，学的东西要写很久。姑且先鸽一会。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="vit">VIT<a class="hash-link" href="#vit" title="Direct link to heading">​</a></h2><p>第一次将transformer引入cv中，因为transformer在NLP中的表现很好。</p><p><img loading="lazy" src="http://tva3.sinaimg.cn/large/008snjoggy1h22il1ij2kj30hg0ljq6x.jpg" alt="image" class="img_E7b_"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="transformer的优势">Transformer的优势<a class="hash-link" href="#transformer的优势" title="Direct link to heading">​</a></h3><p>1.并行计算</p><p><img loading="lazy" src="http://tva1.sinaimg.cn/large/008snjoggy1h22ilzwwb6j30oo0h9wm3.jpg" alt="image" class="img_E7b_">2.全局视野
3.灵活的堆叠能力</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="vit结构">VIT结构<a class="hash-link" href="#vit结构" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="http://tvax2.sinaimg.cn/large/008snjoggy1h22iojnunaj30w10hnn3w.jpg" alt="image" class="img_E7b_"></p><p>只使用了transformer的encoder</p><p>但是transformer有全局视野</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="self-attention">self attention<a class="hash-link" href="#self-attention" title="Direct link to heading">​</a></h4><p>对相似度的计算</p><p><img loading="lazy" src="http://tvax2.sinaimg.cn/large/008snjoggy1h22is7c3bpj30bg02e74h.jpg" alt="image" class="img_E7b_"></p><p><img loading="lazy" src="http://tva2.sinaimg.cn/large/008snjoggy1h22it5kkp9j30f40boq4y.jpg" alt="image" class="img_E7b_"></p><p>Query:查询,询问</p><p>Key:键值,关键词</p><p>Value:价值,数值</p><p>q1，k1向量点积计算相似度，从点积求投影就可以看成，越相似，投影越长。</p><p><img loading="lazy" src="http://tvax2.sinaimg.cn/large/008snjoggy1h22iycgkwnj30fg0btwha.jpg" alt="image" class="img_E7b_"></p><p>norm只是为了让值小一点，multiply我理解为重要程度，而sum就是上下文。</p><p>矩阵计算本身就是并行</p><p>MultiHead Attention
有多个Wq,Wk,Wv,上述操作重复多次结果concat一起</p><p>可以给注意力提供多种可能性，比如稀疏，密集之类</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="transformer-encoder">transformer encoder<a class="hash-link" href="#transformer-encoder" title="Direct link to heading">​</a></h4><p><img loading="lazy" src="http://tvax1.sinaimg.cn/large/008snjoggy1h22j7pdolwj307p0dzgn1.jpg" alt="image" class="img_E7b_"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="输入端适配">输入端适配<a class="hash-link" href="#输入端适配" title="Direct link to heading">​</a></h4><p>因为transformer本身并非用于cv的，而是NLP，所以需要做出一些调整，将图片进行切分然后按编号输入网络。用来模拟句子，当然输入线性层直接打平。</p><p><img loading="lazy" src="http://tvax3.sinaimg.cn/large/008snjoggy1h22jcck0zfj30gd091jtr.jpg" alt="image" class="img_E7b_"></p><h5 class="anchor anchorWithStickyNavbar_mojV" id="patch-0的作用">Patch 0的作用<a class="hash-link" href="#patch-0的作用" title="Direct link to heading">​</a></h5><p>需要一个整合信息的向量
如果只有原始输出的9个向量
用哪个向量来分类都不好全用计算量又很大
所以加一个可学习的vector也就是patch 0来整合信息。</p><p>同时图像切片损失了位置信息，而且transformer本身也不关注这个。所以需要补位置信息。ViT使用了一个可学习的vector来编码
vector和patch vector直接相加组成输入。（相加是concat的特例）</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="pvt">PVT<a class="hash-link" href="#pvt" title="Direct link to heading">​</a></h2><p>挺拉的，就别浪费时间管他了，主要就想控制显存，transformer真不是一般人能用的，控制k，v的数量，但是它怎么控制的我真不想评价，只能说给我干沉默了。</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="swin-transformer">Swin transformer<a class="hash-link" href="#swin-transformer" title="Direct link to heading">​</a></h2><p><code>很好的baseline！</code>它和之前的PVT其实关注的都是一个问题，transformer占显存，通过控制token大小与数量来实现。</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="vit的缺点">VIT的缺点<a class="hash-link" href="#vit的缺点" title="Direct link to heading">​</a></h3><ul><li><p>显存占用- Token数量</p></li><li><p>表示方法-Token应该多大?</p></li><li><p>难以用在下游任务</p><ul><li>Token指的是模型运行中最小的处理单元。</li><li>NLP中的Token一般指的是一个单词，Token的数量就是一段文字中单词的数量，一般不会特别多。</li><li>ViT中的Token指的是一个Patch，Token的数量就是Patch的数量，这一点和图像分辨率以及Patch大小有关</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="dilemma">dilemma<a class="hash-link" href="#dilemma" title="Direct link to heading">​</a></h3><p><strong>Patch小-&gt; Token数量多-&gt;MHA复杂度高-&gt;资源不足</strong></p><p><strong>Patch大-&gt;Token数量少-&gt;Feature Map分辨率低</strong></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="structure">structure<a class="hash-link" href="#structure" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="http://tva1.sinaimg.cn/large/008snjoggy1h25w5xap5tj30u50awwkl.jpg" alt="image" class="img_E7b_"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="w-msa">W-MSA<a class="hash-link" href="#w-msa" title="Direct link to heading">​</a></h3><p>Swin Transformer使用window multiscale self attention，将attention的计算限制在同一个窗口内，使得复杂度降到了o(n)</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="缺点">缺点<a class="hash-link" href="#缺点" title="Direct link to heading">​</a></h4><p>显然这样硬性的限制会丢失全局信息，限制模型能力，因此需要一个跨Window的操作，增强window间的交互，所以还要一个SW-MSA</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="sw-msa">SW-MSA<a class="hash-link" href="#sw-msa" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="http://tva3.sinaimg.cn/large/008snjoggy1h25wca6yc8j30nw09p0xj.jpg" alt="image" class="img_E7b_"></p><h4 class="anchor anchorWithStickyNavbar_mojV" id="缺点-1">缺点<a class="hash-link" href="#缺点-1" title="Direct link to heading">​</a></h4><p>移动切分后，window的数量变多了，而且window的尺寸不一样了。</p><p>所以他们设计了一种更高效的办法<code>cyclic shift</code></p><p><img loading="lazy" src="http://tvax3.sinaimg.cn/large/008snjoggy1h25wftyyesj30ta0640ue.jpg" alt="image" class="img_E7b_"></p><p><img loading="lazy" src="http://tvax4.sinaimg.cn/large/008snjoggy1h25whi7rc9j30uy078go6.jpg" alt="image" class="img_E7b_"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="相对位置编码">相对位置编码<a class="hash-link" href="#相对位置编码" title="Direct link to heading">​</a></h3><p>这里的相对位置编码是对每个window内的patch写死的，是不可学习的，是人为设计好的</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="inductive-bias">inductive bias<a class="hash-link" href="#inductive-bias" title="Direct link to heading">​</a></h3><p>比较有趣的一个东西，它就是我们所说的归纳偏置
归纳-&gt;总结，也就是从现存的例子中找到一些比较通用的规则偏置-&gt;选择偏好。
整理在一起可以理解为:在面对一些特定问题的时候，我们认为模型应该会有哪些特点会比较容易work，因此而做出的一系列对模型的人为限制。
比如图像处理中，每个位置的信息与周围的信息相关，因此设计出conv</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="conv中的两个归纳偏置">conv中的两个归纳偏置<a class="hash-link" href="#conv中的两个归纳偏置" title="Direct link to heading">​</a></h4><ul><li>局部相关性：邻近的像素是相关的。会导致feature map十分平滑，在一定程度上影响目标检测的效果。</li><li>权重共享：图像的不同部分应该以相同的方式处理，无论它们的绝对位置如何。</li></ul><p>比如NLP中，认为输出的结果与单词的先后顺序相关，因此设计出RNN。</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="为什么卷积核是奇数的">为什么卷积核是奇数的<a class="hash-link" href="#为什么卷积核是奇数的" title="Direct link to heading">​</a></h3><p>因为奇数的卷积核拥有中心，可以很好确定位置</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="为什么深层feature-map让人san值狂掉">为什么深层feature map让人san值狂掉<a class="hash-link" href="#为什么深层feature-map让人san值狂掉" title="Direct link to heading">​</a></h3><p>因为浅层特征还关注的是颜色边缘，深层以后其实已经是多个特征组成的语义了。能理解就怪了（）</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="detr">DETR<a class="hash-link" href="#detr" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="structure-1">structure<a class="hash-link" href="#structure-1" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="http://tva3.sinaimg.cn/large/008snjoggy1h25x0q2v5yj30te0a0ag0.jpg" alt="image" class="img_E7b_"></p><p><img loading="lazy" src="http://tva4.sinaimg.cn/large/008snjoggy1h25x1dx7o7j30q20d6aj5.jpg" alt="image" class="img_E7b_"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="nms缺点">nms缺点<a class="hash-link" href="#nms缺点" title="Direct link to heading">​</a></h3><ol><li>如果两个框离的很近，那么两个框很有可能属于同一instance(两个instance本来就很近)</li><li>在属于同一instance的框中，分类得分越高的，定位质量越高（并不一定)</li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="dense-prediction">Dense prediction<a class="hash-link" href="#dense-prediction" title="Direct link to heading">​</a></h3><ol><li>利用Anchor进行预定位</li><li>判断Anchor是前景还是背景给出proposal</li><li>之后预测给出的proposal的类别</li><li>最后NMS输出结果</li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="set-prediction">Set prediction<a class="hash-link" href="#set-prediction" title="Direct link to heading">​</a></h3><p>将预测集与GT集进行匹配。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="match-loss">match loss<a class="hash-link" href="#match-loss" title="Direct link to heading">​</a></h4><p><img loading="lazy" src="http://tvax3.sinaimg.cn/large/008snjoggy1h25x49ci3aj30tn08qgqg.jpg" alt="image" class="img_E7b_"></p><p><img loading="lazy" src="http://tvax4.sinaimg.cn/large/008snjoggy1h25x558qygj30tn0extds.jpg" alt="image" class="img_E7b_"></p><p>匈牙利算法之前deepsort说过，这里就不说了，就是找最大匹配。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="位置编码position-embedding">位置编码（position embedding）<a class="hash-link" href="#位置编码position-embedding" title="Direct link to heading">​</a></h4><p>这里的位置编码是写死的，不可学习的
具体写法是根据左边公式，其中pos代表token在序列中的位置，而d代表一个token用多少维来表示。由于图像是2d的，因此DETR将这种方法推广到了2d，见右边公式。</p><p><img loading="lazy" src="http://tvax2.sinaimg.cn/large/008snjoggy1h25xaka0o5j30dx03oq3a.jpg" alt="image" class="img_E7b_"><img loading="lazy" src="http://tva3.sinaimg.cn/large/008snjoggy1h25xc98c0qj30gr066wfg.jpg" alt="image" class="img_E7b_"></p><p>这里的位置编码会应用到每一个encoder上，而不只是开头的第一个而且只会加到QK上，不影响V
原因: Patch间的信息交互才需要位置信息，也就是需要加到Q@KT这一步上。而下一次生成QK，是根据V生成的，所以每一个encoder都要重新加一遍这个信息。</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="encoderdecoder">encoder&amp;decoder<a class="hash-link" href="#encoderdecoder" title="Direct link to heading">​</a></h4><p><img loading="lazy" src="http://tva2.sinaimg.cn/large/008snjoggy1h25xee0cb7j30e00e50vf.jpg" alt="image" class="img_E7b_"></p><p>左侧为编码器，右侧为解码器
多出来的MSA，K和V来自于encoder而Q是来自于Obj queries
这种来QK来自于不同地方的attention也叫cross attention</p><h5 class="anchor anchorWithStickyNavbar_mojV" id="object-quiries">Object quiries<a class="hash-link" href="#object-quiries" title="Direct link to heading">​</a></h5><p>这个东西的作用和cls token类似，也是在整合信息。
Object queries是一个可学习的向量(num, b, dim)</p><ul><li>Num是人为给的值，远大于图片内物体数量，默认100b是batch size</li><li>dim是attention运行过程中用的维度数</li></ul><p>最终学出来的东西类似于Anchor</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/2022/05/25/BMLCodelab"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">BMLCodelab</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/2022/05/09/win10"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">win10快捷键</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#vit" class="table-of-contents__link toc-highlight">VIT</a><ul><li><a href="#transformer的优势" class="table-of-contents__link toc-highlight">Transformer的优势</a></li><li><a href="#vit结构" class="table-of-contents__link toc-highlight">VIT结构</a></li></ul></li><li><a href="#pvt" class="table-of-contents__link toc-highlight">PVT</a></li><li><a href="#swin-transformer" class="table-of-contents__link toc-highlight">Swin transformer</a><ul><li><a href="#vit的缺点" class="table-of-contents__link toc-highlight">VIT的缺点</a></li><li><a href="#dilemma" class="table-of-contents__link toc-highlight">dilemma</a></li><li><a href="#structure" class="table-of-contents__link toc-highlight">structure</a></li><li><a href="#w-msa" class="table-of-contents__link toc-highlight">W-MSA</a></li><li><a href="#sw-msa" class="table-of-contents__link toc-highlight">SW-MSA</a></li><li><a href="#相对位置编码" class="table-of-contents__link toc-highlight">相对位置编码</a></li><li><a href="#inductive-bias" class="table-of-contents__link toc-highlight">inductive bias</a></li><li><a href="#为什么卷积核是奇数的" class="table-of-contents__link toc-highlight">为什么卷积核是奇数的</a></li><li><a href="#为什么深层feature-map让人san值狂掉" class="table-of-contents__link toc-highlight">为什么深层feature map让人san值狂掉</a></li></ul></li><li><a href="#detr" class="table-of-contents__link toc-highlight">DETR</a><ul><li><a href="#structure-1" class="table-of-contents__link toc-highlight">structure</a></li><li><a href="#nms缺点" class="table-of-contents__link toc-highlight">nms缺点</a></li><li><a href="#dense-prediction" class="table-of-contents__link toc-highlight">Dense prediction</a></li><li><a href="#set-prediction" class="table-of-contents__link toc-highlight">Set prediction</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.6adc9f19.js"></script>
<script src="/assets/js/main.4372d31a.js"></script>
</body>
</html>